---
layout: post
title:  "机器学习——线性回归"
date:   2017-09-05
desc: "Machine learn LinearRegression 线性回归"
keywords: "机器学习,python,介绍"
categories: [Machinelearn]
tags: [机器学习,python,线性回归]
icon: icon-python
---

### 导读
在监督学习过程完整的流程中进行线性回归算法
监督学习：在给出的数据集（每个数据含“正确的答案”)——训练集，预测准确的输出结果
单变量线性回归算法：\\(h_{\theta} (x)=\theta_{0}+\theta_{1}x\\) 

### 单变量线性回归
1. 原型函数
训练集

| 尺寸大小(\\(m^2\\)) | 价格($)  | 
| ----------------    | :----:   | 
| 2104                | 460      |
| 1416                | 232      |
| 1534                | 315      |
| 852                 | 178      |
| ...                 | ...      | 

标记:  
m 代表训练集中实例的数量  
x 代表特征/输入变量  
y 代表目标变量/输出变量  
(x,y) 代表训练集中的实例  
(x(i),y(i) ) 代表第 i 个观察实例  
h 代表学习算法的解决方案或函数也称为假设（hypothesis）  

把训练集喂给学习算法，然后输出一个函数\\(h_{\theta} (x)=\theta_{0}+\theta_{1}x\\)  

2. 代价函数  
模型预测的值与训练集中实际值之间的差距就是建模误差
目标:便是选择出可以使得建模误差的平方和能够最小的模型参数,即代价\\(J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{0}(x^{(i)})-y^{(i)})^2\\)
代价函数也被称作平方误差函数，有时也被称为平方误差代价函数
3. 梯度下降  
梯度下降(\\(\theta_{j} :=\theta_{j}-\alpha\frac{\sigma}{\sigma\theta_{j}}J(\theta) \\)  α 是学习率)是一个用来求函数最小值的算法
思想：开始时我们随机选择一个参数的组合（θ0,θ1,...,θn），计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合。直到一个局部最小值(不一定是全局最小值)
在局部最低时导数等于零：代价函数导数为0，即为代价函数最低点






